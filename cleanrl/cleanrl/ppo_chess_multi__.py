import argparse
import time
from dataclasses import dataclass

import gymnasium as gym
from gymnasium import spaces
from gymnasium.wrappers import RecordVideo
from gymnasium.vector import AsyncVectorEnv

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

from tqdm import tqdm
from pettingzoo.classic import chess_v6

import os
import wandb
import time


def record_chess_video(agent, device, video_dir, update_idx, max_steps=300):
    """
    í˜„ì¬ agent policyë¡œ í•œ íŒ(or max_steps) ë‘ë©´ì„œ videoë¥¼ ì €ì¥.
    video_dir/update_{update_idx}.* ë¡œ íŒŒì¼ ìƒì„±ë¨.
    """
    os.makedirs(video_dir, exist_ok=True)

    name_prefix = f"update_{update_idx}"

    # rgb_array ëª¨ë“œë¡œ ë‹¨ì¼ env ìƒì„± + RecordVideo ë˜í¼
    eval_env = ChessSelfPlayEnv(render_mode="rgb_array")
    eval_env = RecordVideo(
        eval_env,
        video_folder=video_dir,
        episode_trigger=lambda ep_id: True,  # ì²« ì—í”¼ì†Œë“œë§Œ ì €ì¥
        name_prefix=name_prefix,
    )

    agent.eval()  # eval ëª¨ë“œ

    obs, info = eval_env.reset()
    obs_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)
    mask_t = torch.tensor(
        info["action_mask"], dtype=torch.float32, device=device
    ).unsqueeze(0)

    done = False
    step = 0

    while not done and step < max_steps:
        with torch.no_grad():
            action, _, _, _ = agent.get_action_and_value(obs_t, action_mask=mask_t)
        action_id = action.item()

        obs, reward, terminated, truncated, info = eval_env.step(action_id)
        done = terminated or truncated

        obs_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)
        mask_t = torch.tensor(
            info["action_mask"], dtype=torch.float32, device=device
        ).unsqueeze(0)
        step += 1

    eval_env.close()
    agent.train()  # ë‹¤ì‹œ train ëª¨ë“œ


# ==============================
#  ChessSelfPlayEnv (PettingZoo -> Gymnasium ë‹¨ì¼ env)
#  í•œ ìª½ì€ PPO, ë‹¤ë¥¸ ìª½ì€ í•­ìƒ ëœë¤ìœ¼ë¡œ ë‘ëŠ” ë²„ì „
# ==============================
import chess


class ChessSelfPlayEnv(gym.Env):
    metadata = {
        "render_modes": ["human", "ansi", "rgb_array"],
        "name": "ChessSelfPlayEnv",
    }

    def __init__(self, render_mode=None):
        super().__init__()
        self.render_mode = render_mode
        self.aec_env = chess_v6.env(render_mode=render_mode)
        self.aec_env.reset()

        agents = self.aec_env.agents
        self.agent_id = agents[0]  # PPO (Whiteë¼ê³  ê°€ì •)
        self.opponent_id = agents[1]  # Random (Black)

        # Observation / Action Space ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
        raw_obs_space = self.aec_env.observation_space(self.agent_id)["observation"]
        self.observation_space = spaces.Box(
            low=raw_obs_space.low,
            high=raw_obs_space.high,
            shape=raw_obs_space.shape,
            dtype=np.float32,
        )
        self.action_space = self.aec_env.action_space(self.agent_id)

        # [Reward Shaping] ì´ì „ í„´ì˜ ê¸°ë¬¼ ì ìˆ˜ ì°¨ì´ë¥¼ ì €ì¥í•  ë³€ìˆ˜
        self.last_board_value = 0.0

    def _get_board_value(self):
        """
        í˜„ì¬ ë³´ë“œ ìƒíƒœì—ì„œ (ë‚´ ê¸°ë¬¼ ì ìˆ˜ - ìƒëŒ€ ê¸°ë¬¼ ì ìˆ˜)ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜.
        PettingZooì˜ ë‚´ë¶€ chess.Board ê°ì²´ì— ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        # PettingZoo chess_v6ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ python-chessì˜ board ê°ì²´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
        # êµ¬ì¡°: aec_env -> env -> env -> board (ì ‘ê·¼ ê²½ë¡œê°€ ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ unwrapped ì‚¬ìš©)
        board = self.aec_env.unwrapped.board

        # ê¸°ë¬¼ ì ìˆ˜ ë§¤í•‘ (ì¼ë°˜ì ì¸ ì²´ìŠ¤ ì ìˆ˜)
        piece_values = {
            chess.PAWN: 1,
            chess.KNIGHT: 3,
            chess.BISHOP: 3,
            chess.ROOK: 5,
            chess.QUEEN: 9,
            chess.KING: 0,  # í‚¹ì€ ì¡íˆì§€ ì•Šìœ¼ë¯€ë¡œ 0 (í˜¹ì€ ì²´í¬ë©”ì´íŠ¸ ë³´ìƒìœ¼ë¡œ ëŒ€ì²´ë¨)
        }

        value = 0.0
        # board.piece_map()ì€ {square_index: Pieceê°ì²´} ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        for piece in board.piece_map().values():
            score = piece_values.get(piece.piece_type, 0)

            # ë‚´ ê¸°ë¬¼ì´ë©´ +, ìƒëŒ€ ê¸°ë¬¼ì´ë©´ -
            # self.agent_idê°€ "player_0" (White)ë¼ê³  ê°€ì •
            if self.agent_id == "player_0":
                if piece.color == chess.WHITE:
                    value += score
                else:
                    value -= score
            else:  # ë‚´ê°€ Blackì¸ ê²½ìš°
                if piece.color == chess.BLACK:
                    value += score
                else:
                    value -= score

        return value

    def reset(self, seed=None, options=None):
        self.aec_env.reset(seed=seed)

        # [Reward Shaping] ì´ˆê¸° ë³´ë“œ ì ìˆ˜ (ë³´í†µ 0)
        self.last_board_value = 0.0

        # ìƒëŒ€ í„´ì´ë©´ ëœë¤ìœ¼ë¡œ ì§„í–‰ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        while self.aec_env.agent_selection != self.agent_id:
            curr = self.aec_env.agent_selection
            obs_opp = self.aec_env.observe(curr)
            mask_opp = obs_opp["action_mask"]
            action_opp = self.aec_env.action_space(curr).sample(mask_opp)
            self.aec_env.step(int(action_opp))
            if all(self.aec_env.terminations.values()) or all(
                self.aec_env.truncations.values()
            ):
                break

        # Reset ì§í›„ ë‚´ ê¸°ë¬¼ ì ìˆ˜ ê³„ì‚° (ì´ˆê¸°í™”)
        self.last_board_value = self._get_board_value()

        # ê´€ì¸¡ ë°˜í™˜ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        if not (
            all(self.aec_env.terminations.values())
            or all(self.aec_env.truncations.values())
        ):
            raw_obs = self.aec_env.observe(self.agent_id)
            obs = raw_obs["observation"].astype(np.float32)
            action_mask = raw_obs["action_mask"].astype(np.float32)
        else:
            obs = np.zeros(self.observation_space.shape, dtype=np.float32)
            action_mask = np.ones(self.action_space.n, dtype=np.float32)

        return obs, {"action_mask": action_mask}

    def step(self, action):
        # 1. ìš°ë¦¬ ì—ì´ì „íŠ¸ ìˆ˜ ë‘ê¸°
        self.aec_env.step(int(action))

        terminated = all(self.aec_env.terminations.values())
        truncated = all(self.aec_env.truncations.values())
        done = terminated or truncated

        # 2. ìƒëŒ€(ëœë¤) ìˆ˜ ë‘ê¸° (ê²Œì„ ì•ˆ ëë‚¬ìœ¼ë©´)
        if not done:
            obs_opp = self.aec_env.observe(self.opponent_id)
            mask_opp = obs_opp["action_mask"]
            action_opp = self.aec_env.action_space(self.opponent_id).sample(mask_opp)
            self.aec_env.step(int(action_opp))

            terminated = all(self.aec_env.terminations.values())
            truncated = all(self.aec_env.truncations.values())
            done = terminated or truncated

        # 3. ë³´ìƒ ê³„ì‚° (ê¸°ì¡´ ìŠ¹íŒ¨ ë³´ìƒ + Shaping ë³´ìƒ)
        original_reward = float(self.aec_env.rewards[self.agent_id])

        # [Reward Shaping] í˜„ì¬ ë³´ë“œ ì ìˆ˜ ê³„ì‚°
        current_board_value = self._get_board_value()

        # ì ìˆ˜ ë³€í™”ëŸ‰ (ë‚´ ì ìˆ˜ê°€ ëŠ˜ê±°ë‚˜, ìƒëŒ€ ì ìˆ˜ê°€ ì¤„ë©´ ì´ë“)
        # ì˜ˆ: ë‚´ê°€ ìƒëŒ€ í°ì„ ì¡ìŒ -> (ë‚´ì ìˆ˜ - (ìƒëŒ€ì ìˆ˜-1)) - (ë‚´ì ìˆ˜ - ìƒëŒ€ì ìˆ˜) = +1
        shaping_reward = current_board_value - self.last_board_value

        # [ì¤‘ìš”] Shaping ê³„ìˆ˜ ì¡°ì ˆ (Coefficient)
        # ê¸°ë¬¼ ì ìˆ˜ 1ì ì´ ìŠ¹ë¦¬(1.0)ë³´ë‹¤ í¬ë©´ ì•ˆ ë˜ë¯€ë¡œ, ì ì ˆíˆ ì¤„ì—¬ì¤ë‹ˆë‹¤.
        # ì˜ˆ: í° í•˜ë‚˜ ì¡ëŠ” ê²ƒ = 0.02ì  (í° 50ê°œ ì¡ì•„ì•¼ ìŠ¹ë¦¬ 1ì ê³¼ ë§ë¨¹ìŒ -> ìŠ¹ë¦¬ê°€ ë” ì¤‘ìš”í•¨ì„ ìœ ì§€)
        shaping_coeff = 0.02

        total_reward = original_reward + (shaping_reward * shaping_coeff)

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        self.last_board_value = current_board_value

        # 4. ë‹¤ìŒ ê´€ì¸¡ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        if not done:
            raw_obs = self.aec_env.observe(self.agent_id)
            obs = raw_obs["observation"].astype(np.float32)
            action_mask = raw_obs["action_mask"].astype(np.float32)
        else:
            obs = np.zeros(self.observation_space.shape, dtype=np.float32)
            action_mask = np.ones(self.action_space.n, dtype=np.float32)

        return obs, total_reward, terminated, truncated, {"action_mask": action_mask}

    def render(self):
        # PettingZoo í™˜ê²½ì˜ renderë¥¼ ê·¸ëŒ€ë¡œ í˜¸ì¶œ
        return self.aec_env.render()

    def close(self):
        self.aec_env.close()


# ==============================
#  PPO ë„¤íŠ¸ì›Œí¬ & í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ==============================


class ActorCritic(nn.Module):
    def __init__(self, obs_shape, n_actions):
        super().__init__()
        self.h, self.w, self.c = obs_shape

        self.network = nn.Sequential(
            nn.Conv2d(self.c, 32, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(128 * self.h * self.w, 512),
            nn.ReLU(),
        )

        self.actor = nn.Linear(512, n_actions)
        self.critic = nn.Linear(512, 1)

    def get_value(self, obs):
        obs = obs.permute(0, 3, 1, 2)  # (B, H, W, C) -> (B, C, H, W)
        hidden = self.network(obs)
        return self.critic(hidden).squeeze(-1)  # (B,)

    def get_action_and_value(self, obs, action=None, action_mask=None):
        obs = obs.permute(0, 3, 1, 2)  # (B, H, W, C) -> (B, C, H, W)
        hidden = self.network(obs)
        logits = self.actor(hidden)  # (B, n_actions)

        if action_mask is not None:
            # illegal moveì˜ logitì„ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ â†’ ì„ íƒ ë¶ˆê°€
            logits = logits.masked_fill(action_mask == 0, -1e9)

        dist = torch.distributions.Categorical(logits=logits)

        if action is None:
            action = dist.sample()  # (B,)

        log_prob = dist.log_prob(action)  # (B,)
        entropy = dist.entropy()  # (B,)
        value = self.critic(hidden).squeeze(-1)  # (B,)

        return action, log_prob, entropy, value


@dataclass
class PPOConfig:
    exp_name: str = "ppo_pettingzoo_chess_vector"
    seed: int = 1
    total_timesteps: int = 5_000_000
    learning_rate: float = 2.5e-4
    num_steps: int = 512  # rollout horizon
    num_envs: int = 32  # ë³‘ë ¬ í™˜ê²½ ìˆ˜
    gamma: float = 0.99
    gae_lambda: float = 0.95
    update_epochs: int = 10
    clip_coef: float = 0.2
    ent_coef: float = 0.01
    vf_coef: float = 0.5
    max_grad_norm: float = 0.5
    minibatch_size: int = 2048  # (num_steps * num_envs) ê¸°ì¤€
    logging: bool = True


# ==============================
#  ë²¡í„° í™˜ê²½ ìƒì„± (AsyncVectorEnv ì‚¬ìš©)
# ==============================


def make_single_env(seed_offset=0):
    def _init():
        env = ChessSelfPlayEnv(render_mode=None)
        env.reset(seed=seed_offset)
        return env

    return _init


def make_vector_env(num_envs):
    return AsyncVectorEnv([make_single_env(seed_offset=i) for i in range(num_envs)])


# ==============================
#  PPO í•™ìŠµ ë£¨í”„
# ==============================


def train(config: PPOConfig):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[INFO] Using device: {device}")

    if config.logging:
        run = wandb.init(
            # Set the wandb entity where your project will be logged (generally your team name).
            entity="edenkim9741-chonnam-national-university",
            # Set the wandb project where this run will be logged.
            project="Reinforcement-Learning",
            name=time.strftime("%Y-%m-%d_%H-%M-%S") + "_ppo_chess_selfplay",
            # Track hyperparameters and run metadata.
            config={
                "architecture": "CNN",
                "dataset": "Chess",
            },
        )

    env = make_vector_env(config.num_envs)
    obs_shape = env.single_observation_space.shape
    n_actions = env.single_action_space.n
    num_envs = config.num_envs

    agent = ActorCritic(obs_shape, n_actions).to(device)
    optimizer = optim.Adam(agent.parameters(), lr=config.learning_rate, eps=1e-5)

    # Rollout buffers: (T, E, ...)
    obs_buf = torch.zeros(
        (config.num_steps, num_envs) + obs_shape, dtype=torch.float32, device=device
    )
    masks_buf = torch.zeros(
        (config.num_steps, num_envs, n_actions), dtype=torch.float32, device=device
    )
    actions_buf = torch.zeros(
        (config.num_steps, num_envs), dtype=torch.long, device=device
    )
    logprobs_buf = torch.zeros(
        (config.num_steps, num_envs), dtype=torch.float32, device=device
    )
    rewards_buf = torch.zeros(
        (config.num_steps, num_envs), dtype=torch.float32, device=device
    )
    dones_buf = torch.zeros(
        (config.num_steps, num_envs), dtype=torch.float32, device=device
    )
    values_buf = torch.zeros(
        (config.num_steps, num_envs), dtype=torch.float32, device=device
    )
    advantages = torch.zeros(
        (config.num_steps, num_envs), dtype=torch.float32, device=device
    )

    # ì—í”¼ì†Œë“œ ë¦¬í„´ í†µê³„
    episode_returns = []
    current_ep_return = np.zeros(num_envs, dtype=np.float32)

    # Reset vector env
    next_obs_np, info = env.reset()
    next_obs = torch.tensor(
        next_obs_np, dtype=torch.float32, device=device
    )  # (E, *obs)
    next_mask = torch.tensor(
        info["action_mask"], dtype=torch.float32, device=device
    )  # (E, A)
    next_done = torch.zeros((num_envs,), dtype=torch.float32, device=device)

    global_step = 0
    # í•œ updateë‹¹ ìˆ˜ì§‘ë˜ëŠ” transition ìˆ˜: num_steps * num_envs
    num_updates = config.total_timesteps // (config.num_steps * num_envs)

    start_time = time.time()

    bar = tqdm(range(1, num_updates + 1), desc="PPO Training")
    for update in bar:
        # ==========================
        # Rollout ìˆ˜ì§‘
        # ==========================
        for step in range(config.num_steps):
            global_step += num_envs  # envê°€ Eê°œì´ë¯€ë¡œ

            obs_buf[step] = next_obs
            masks_buf[step] = next_mask
            dones_buf[step] = next_done

            with torch.no_grad():
                actions, logprobs, entropy, values = agent.get_action_and_value(
                    next_obs, action_mask=next_mask
                )
                # actions, logprobs, values: (E,)

            actions_buf[step] = actions
            logprobs_buf[step] = logprobs
            values_buf[step] = values

            # ë²¡í„° env step
            actions_np = actions.cpu().numpy()
            obs_step, rewards, terminated, truncated, infos = env.step(actions_np)
            done = np.logical_or(terminated, truncated)  # (E,)

            rewards_buf[step] = torch.tensor(
                rewards, device=device, dtype=torch.float32
            )
            current_ep_return += rewards.astype(np.float32)
            if config.logging:
                run.log({"train/step_reward": np.mean(rewards), "global_step": global_step})

            # doneì¸ envë“¤ ë¦¬í„´ë§Œ ê¸°ë¡ (AsyncVectorEnvëŠ” auto-resetë¡œ ìƒˆ episode ì‹œì‘)
            if np.any(done):
                for i in range(num_envs):
                    if done[i]:
                        episode_returns.append(current_ep_return[i])
                        current_ep_return[i] = 0.0

            # ë‹¤ìŒ ìƒíƒœ ì¤€ë¹„
            next_obs = torch.tensor(obs_step, dtype=torch.float32, device=device)
            next_mask = torch.tensor(
                infos["action_mask"], dtype=torch.float32, device=device
            )
            next_done = torch.tensor(done, dtype=torch.float32, device=device)

        # ==========================
        # GAE advantage ê³„ì‚°
        # ==========================
        with torch.no_grad():
            next_value = agent.get_value(next_obs)  # (E,)

        lastgaelam = torch.zeros(num_envs, dtype=torch.float32, device=device)
        for t in reversed(range(config.num_steps)):
            if t == config.num_steps - 1:
                next_nonterminal = 1.0 - next_done  # (E,)
                next_values = next_value  # (E,)
            else:
                next_nonterminal = 1.0 - dones_buf[t + 1]  # (E,)
                next_values = values_buf[t + 1]  # (E,)

            delta = (
                rewards_buf[t]
                + config.gamma * next_values * next_nonterminal
                - values_buf[t]
            )
            lastgaelam = (
                delta + config.gamma * config.gae_lambda * next_nonterminal * lastgaelam
            )
            advantages[t] = lastgaelam

        returns = advantages + values_buf  # (T, E)

        # ==========================
        # PPO ì—…ë°ì´íŠ¸
        # ==========================
        # (T, E, ...) -> (T*E, ...)
        T, E = config.num_steps, num_envs
        batch_size = T * E

        b_obs = obs_buf.reshape((-1,) + obs_shape)
        b_actions = actions_buf.reshape(batch_size)
        b_logprobs = logprobs_buf.reshape(batch_size)
        b_advantages = advantages.reshape(batch_size)
        b_returns = returns.reshape(batch_size)
        b_values = values_buf.reshape(batch_size)
        b_masks = masks_buf.reshape(batch_size, n_actions)

        inds = np.arange(batch_size)

        # ì†ì‹¤ í†µê³„ìš© ëˆ„ì  ë³€ìˆ˜
        total_loss = 0.0
        total_pg_loss = 0.0
        total_v_loss = 0.0
        total_entropy = 0.0
        n_minibatches = 0

        for epoch in range(config.update_epochs):
            np.random.shuffle(inds)
            for start in range(0, batch_size, config.minibatch_size):
                end = start + config.minibatch_size
                mb_inds = inds[start:end]

                mb_obs = b_obs[mb_inds]
                mb_actions = b_actions[mb_inds]
                mb_logprobs_old = b_logprobs[mb_inds]
                mb_advantages = b_advantages[mb_inds]
                mb_returns = b_returns[mb_inds]
                mb_values_old = b_values[mb_inds]
                mb_masks = b_masks[mb_inds]

                _, newlogprob, entropy, value = agent.get_action_and_value(
                    mb_obs, mb_actions, action_mask=mb_masks
                )
                ratio = (newlogprob - mb_logprobs_old).exp()

                # Normalize advantage
                mb_advantages = (mb_advantages - mb_advantages.mean()) / (
                    mb_advantages.std() + 1e-8
                )

                # Policy loss (clipped surrogate)
                pg_loss1 = -mb_advantages * ratio
                pg_loss2 = -mb_advantages * torch.clamp(
                    ratio, 1.0 - config.clip_coef, 1.0 + config.clip_coef
                )
                pg_loss = torch.max(pg_loss1, pg_loss2).mean()

                # Value loss (clipped)
                v_loss_unclipped = (value - mb_returns) ** 2
                v_clipped = mb_values_old + torch.clamp(
                    value - mb_values_old, -config.clip_coef, config.clip_coef
                )
                v_loss_clipped = (v_clipped - mb_returns) ** 2
                v_loss = 0.5 * torch.max(v_loss_unclipped, v_loss_clipped).mean()

                entropy_loss = entropy.mean()
                loss = (
                    pg_loss - config.ent_coef * entropy_loss + config.vf_coef * v_loss
                )

                optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(agent.parameters(), config.max_grad_norm)
                optimizer.step()

                # í†µê³„ ëˆ„ì 
                total_loss += loss.item()
                total_pg_loss += pg_loss.item()
                total_v_loss += v_loss.item()
                total_entropy += entropy_loss.item()
                n_minibatches += 1

        # í•œ update ê¸°ì¤€ í‰ê·  ì†ì‹¤
        mean_loss = total_loss / max(1, n_minibatches)
        mean_pg_loss = total_pg_loss / max(1, n_minibatches)
        mean_v_loss = total_v_loss / max(1, n_minibatches)
        mean_entropy = total_entropy / max(1, n_minibatches)

        # ìµœê·¼ ì—í”¼ì†Œë“œ ë¦¬í„´ (ì˜ˆ: ë§ˆì§€ë§‰ 20ê°œ í‰ê· )
        if len(episode_returns) > 0:
            mean_ep_ret = float(np.mean(episode_returns[-20:]))
        else:
            mean_ep_ret = 0.0

        fps = int(global_step / (time.time() - start_time + 1e-8))

        # ğŸ”¹ tqdm barì— ì •ë³´ í‘œì‹œ (ì´ì „ ìŠ¤íƒ€ì¼ ìœ ì§€)
        bar.set_postfix(
            {
                "ep_ret": f"{mean_ep_ret:.2f}",
                "loss": f"{mean_loss:.3f}",
                "pg": f"{mean_pg_loss:.3f}",
                "v": f"{mean_v_loss:.3f}",
                "ent": f"{mean_entropy:.3f}",
                "fps": fps,
            }
        )

        # ğŸ”¹ 10 updateë§ˆë‹¤ ë¹„ë””ì˜¤ ì €ì¥ (ìš°ë¦¬ vs ëœë¤)
        if update % 100 == 0:
            record_chess_video(
                agent=agent,
                device=device,
                video_dir="videos/chess",
                update_idx=update,
                max_steps=300,
            )

    env.close()
    torch.save(agent.state_dict(), f"{config.exp_name}_final.pt")
    print(f"[INFO] Training finished. Model saved to {config.exp_name}_final.pt")


# ==============================
#  main
# ==============================


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--total-timesteps", type=int, default=40_000_000)
    parser.add_argument("--seed", type=int, default=1)
    parser.add_argument("--num-envs", type=int, default=64)
    parser.add_argument("--num-steps", type=int, default=512)
    parser.add_argument("--logging", type=bool, default=False)

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    cfg = PPOConfig(
        total_timesteps=args.total_timesteps,
        seed=args.seed,
        num_envs=args.num_envs,
        num_steps=args.num_steps,
        logging=args.logging,)

    np.random.seed(cfg.seed)
    torch.manual_seed(cfg.seed)

    train(cfg)
