\documentclass[11pt]{article}

\usepackage{./mystyle}

\doublespacing


\author{Gunhyeong Kim}
\title{Pseudo Exam}
\date{}

\begin{document}
\maketitle
\section{Overview of Reinforcement Learning}
\subsection{Write the 4 Characteristics of Reinforcement Learning}
\vspace{7cm}

\subsection{Write the 4 Example of Reinforcement Learning Applications}
\vspace{7cm}
\clearpage
\subsection{Explain the Definition of Reward Hypothesis}
\vspace{7cm}

\subsection{What is the Sequential Decision Making? Explain about its goal.}
\vspace{7cm}

\subsection{Explain the Differences between Observation and State}
\vspace{7cm}
\clearpage

\subsection{Insert the collect word in the blank}

At each step $t$ the agent:
\begin{itemize}
    \item Executes \_\_\_\_\_\_\_\_\_\_
    \item Receives \_\_\_\_\_\_\_\_\_\_
    \item Receives \_\_\_\_\_\_\_\_\_\_
\end{itemize}

The enviornment:
\begin{itemize}
    \item Receives \_\_\_\_\_\_\_\_\_\_
    \item Emits \_\_\_\_\_\_\_\_\_\_
    \item Emits \_\_\_\_\_\_\_\_\_\_
\end{itemize}

\subsection{Write the Definition of state $S_t$ is Markov}
\vspace{7cm}

\subsection{Fully Observable Environment와 Partially Observable Environment의 차이를 수식으로 설명하시오.}
\vspace{7cm}

\clearpage

\subsection{어떤 Policy $\pi$에서 state $s$에 대한 Value function을 수식으로 쓰시오. (discount factor $\gamma$ 포함)}
\vspace{7cm}

\subsection{state $s$에서 state $s'$로의 Transition Probability를 수식으로 쓰시오. (action $a$ 포함)}
\vspace{7cm}

\subsection{state $s$에서 action $a$를 했을 때 받는 Reward의 기대값을 수식으로 쓰시오.}
\vspace{7cm}

\clearpage

\subsection{Value Based와 Policy Based의 장단점에 대해 서술하시오.}
\vspace{7cm}

\subsection{다음 figure를 보고 uniform random policy의 Value function을 구하시오.}
\begin{figure}[!ht]
    \centering
    \includegraphics[height=7cm]{./figures/Gridworld_Example.png}
    \caption{Gridworld Example}
    \label{fig:gridworld_example}
\end{figure}

\clearpage
\subsection{다음 figure를 보고 optimal value function과 optimal policy를 구하시오.}
\begin{figure}[!ht]
    \centering
    \includegraphics[height=6cm]{./figures/Gridworld_Example_optimal.png}
    \caption{Gridworld Example for Optimal Value Function and Policy}
    \label{fig:gridworld_example_optimal}
\end{figure}

\clearpage
\section{Markov Decision Processes}
\subsection{Write the Definition of Markov}
\vspace{7cm}


\subsection{Write the Definition of Markov Process}
\vspace{7cm}

\subsection{Write the Definition of Markov Reward Process}
\vspace{7cm}

\clearpage
\subsection{Write the Definition of Return $G_t$}
\vspace{7cm}

\subsection{Write the Definition of state-value function of MRP}
\vspace{7cm}

\subsection{Input the collect value in the blank}
\begin{figure}[!ht]
    \centering
    \includegraphics[height=7cm]{./figures/MRP_blank.png}
    \caption{Markov Reward Process}
    \label{fig:markov_reward_process}
\end{figure}
\clearpage

\subsection{Write the Bellman Equation for state-value function of MRP (and also in model based form)}
\vspace{7cm}

\subsection{Solve the Bellman Equation, and Explain why this solution is not practical in real-world applications.}
\vspace{7cm}

\subsection{Write the Definition of Markov Decision Process}
\vspace{7cm}
\clearpage

\subsection{Write the Definition of policy $\pi$ in MDP(contains what it outputs)}
\vspace{6cm}

\subsection{Write the state-value function and action-value function in MDP under policy $\pi$}
\vspace{6cm}

\subsection{Insert the collect value in the blank}
\begin{figure}[!ht]
    \centering
    \includegraphics[height=7cm]{./figures/MDP_blank.png}
    \caption{Markov Decision Process}
    \label{fig:markov_decision_process}
\end{figure}

\clearpage

\subsection{Write the Bellman Expectation Equation for $V^{\pi}$ with diagram}
\vspace{5cm}

\subsection{Write the Bellman Expectation Equation for $Q^{\pi}$ with diagram}
\vspace{5cm}

\subsection{Write the Bellman Expectation Equation for $V^{\pi}$ using $Q^{\pi}$ (with diagram)}
\vspace{5cm}

\subsection{Write the Bellman Expectation Equation for $Q^{\pi}$ using $V^{\pi}$ (with diagram)}

\clearpage

\subsection{Write the Definition of Optimal state-value function $V^*$ and Optimal action-value function $Q^*$}
\vspace{7cm}

\subsection{Write the Theorem of Optimality between $\pi_*$ and $\pi$}
\vspace{7cm}

\subsection{Write the $\pi_*(a \mid s)$ by using $q_*(s,a)$}

\clearpage

\subsection{Write the Optimal values under the Actions}
\begin{figure}[!ht]
    \centering
    \includegraphics[height=10cm]{./figures/Optimal_Value_Function.png}
    \caption{Optimal values under the Actions}
    \label{fig:optimal_values_under_actions}
\end{figure}
\clearpage

\subsection{Write the Bellman Optimality Equation for $V^{\pi}$ with diagram}
\vspace{5cm}

\subsection{Write the Bellman Optimality Equation for $Q^{\pi}$ with diagram}
\vspace{5cm}

\subsection{Write the Bellman Optimality Equation for $V^{\pi}$ using $Q^{\pi}$ (with diagram)}
\vspace{5cm}

\subsection{Write the Bellman Optimality Equation for $Q^{\pi}$ using $V^{\pi}$ (with diagram)}

\clearpage
\section{Planning by Dynamic Programming}
\subsection{What is Dynamic Programming? and What are the two properties of problems that DP can be applied to?}
\vspace{7cm}

\subsection{Explain Policy Iteration. Draw a diagram that shows the process.}
\vspace{7cm}

\subsection{Explain Value Iteration. Write down the value function update rule.}
\vspace{7cm}

\clearpage
\subsection{Compare and contrast Policy Iteration and Value Iteration.}
\vspace{7cm}

\subsection{What is the difference between synchronous and asynchronous dynamic programming?}
\vspace{7cm}

\subsection{Explain Contraction Mapping Theorem and why it is important in Dynamic Programming.}
\vspace{7cm}

\clearpage
\section{Model-Free Prediction}
\subsection{What is the key difference between model-based and model-free reinforcement learning?}
\vspace{7cm}

\subsection{Explain Monte-Carlo (MC) Policy Evaluation. What is the difference between first-visit and every-visit MC?}
\vspace{7cm}

\subsection{Explain Temporal-Difference (TD) Learning. Write down the TD(0) update rule.}
\vspace{7cm}

\clearpage
\subsection{Discuss the Bias-Variance Trade-Off between Monte-Carlo and Temporal-Difference Learning.}
\vspace{7cm}

\subsection{What is TD($\lambda$)? Explain the role of eligibility traces.}
\vspace{7cm}

\subsection{What is the difference between bootstrapping and sampling? Describe MC, TD, and DP in terms of these concepts.}
\vspace{7cm}

\clearpage
\section{Model-Free Control}
\subsection{What is the difference between On-policy and Off-policy learning?}
\vspace{7cm}

\subsection{Explain the concept of $\epsilon$-greedy exploration.}
\vspace{7cm}

\subsection{Explain the Sarsa algorithm. Write down the update rule for the action-value function.}
\vspace{7cm}

\clearpage
\subsection{Explain the Q-learning algorithm. Write down the update rule for the action-value function.}
\vspace{7cm}

\subsection{What is the key difference between Sarsa and Q-learning?}
\vspace{7cm}

\subsection{Explain Importance Sampling for Off-Policy Monte-Carlo and Off-Policy TD.}
\vspace{7cm}


\end{document}