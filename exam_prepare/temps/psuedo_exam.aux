\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Overview of Reinforcement Learning}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Write the 4 Characteristics of Reinforcement Learning}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Write the 4 Example of Reinforcement Learning Applications}{1}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Explain the Definition of Reward Hypothesis}{2}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}What is the Sequential Decision Making? Explain about its goal.}{2}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Explain the Differences between Observation and State}{2}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Insert the collect word in the blank}{3}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Write the Definition of state $S_t$ is Markov}{3}{subsection.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Fully Observable Environment와 Partially Observable Environment의 차이를 수식으로 설명하시오.}{3}{subsection.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}어떤 Policy $\pi $에서 state $s$에 대한 Value function을 수식으로 쓰시오. (discount factor $\gamma $ 포함)}{4}{subsection.1.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10}state $s$에서 state $s'$로의 Transition Probability를 수식으로 쓰시오. (action $a$ 포함)}{4}{subsection.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11}state $s$에서 action $a$를 했을 때 받는 Reward의 기대값을 수식으로 쓰시오.}{4}{subsection.1.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12}Value Based와 Policy Based의 장단점에 대해 서술하시오.}{5}{subsection.1.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.13}다음 figure를 보고 uniform random policy의 Value function을 구하시오.}{5}{subsection.1.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Gridworld Example}}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:gridworld_example}{{1}{5}{Gridworld Example}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.14}다음 figure를 보고 optimal value function과 optimal policy를 구하시오.}{6}{subsection.1.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Gridworld Example for Optimal Value Function and Policy}}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:gridworld_example_optimal}{{2}{6}{Gridworld Example for Optimal Value Function and Policy}{figure.caption.2}{}}
\xdef \mintedoldcachechecksum{\detokenize{\minted@cachechecksum }}
\gdef \@abspage@last{6}
